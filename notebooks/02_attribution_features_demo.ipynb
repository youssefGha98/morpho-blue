{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74b04b6d",
   "metadata": {},
   "source": [
    "# Morpho Blue Attribution Features Demo\n",
    "\n",
    "This notebook demonstrates the attribution layer that decomposes utilization dynamics into flow contributions.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- **Flow Decomposition**: How borrows, repays, liquidations, supplies, and withdrawals contribute to market dynamics\n",
    "- **Utilization Attribution**: Quantify the impact of each flow type on utilization changes\n",
    "- **IRM Diagnostics**: Analyze Interest Rate Model responsiveness\n",
    "- **Rolling Windows**: Multi-timeframe analysis from 5min to 365 days\n",
    "- **Data Quality**: Validate attribution accuracy with integrity checks\n",
    "\n",
    "## Requirements\n",
    "\n",
    "Make sure you have:\n",
    "- Morpho Blue market data in `data_examples/` directory\n",
    "- All dependencies installed via `rye sync`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f883e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imports successful\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "src_path = Path.cwd().parent / \"src\"\n",
    "\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from morpho_blue.indicator_service import IndicatorService\n",
    "from morpho_blue.data.parquet_repository import ParquetRepository\n",
    "from morpho_blue.transformations import AttributionWindowSpec\n",
    "\n",
    "print(\"✓ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ca2008",
   "metadata": {},
   "source": [
    "## Setup: Load Data and Initialize Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfecc07d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data root: /home/youssef/morpho_blue/data_examples\n",
      "Market ID: 0x9103c3b4e834476c9a62ea009ba2c884ee42e94e6e314a26f04d312434191836\n"
     ]
    }
   ],
   "source": [
    "# Market ID for cbBTC market (from your example)\n",
    "CBBTC_MARKET_ID = \"0x9103c3b4e834476c9a62ea009ba2c884ee42e94e6e314a26f04d312434191836\"\n",
    "\n",
    "# Data directory\n",
    "DATA_ROOT = Path.cwd().parent / \"data_examples\"\n",
    "\n",
    "print(f\"Data root: {DATA_ROOT}\")\n",
    "print(f\"Market ID: {CBBTC_MARKET_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b84293cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Service initialized\n"
     ]
    }
   ],
   "source": [
    "# Initialize DuckDB connection\n",
    "con = duckdb.connect()\n",
    "\n",
    "# Initialize repository\n",
    "repo = ParquetRepository(con=con, root=str(DATA_ROOT))\n",
    "\n",
    "# Initialize service\n",
    "service = IndicatorService(repo=repo, con=con)\n",
    "\n",
    "print(\"✓ Service initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db8977d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Market dataset built\n"
     ]
    }
   ],
   "source": [
    "ledger = service.build_market_dataset(market_id=CBBTC_MARKET_ID)\n",
    "print(f\"✓ Market dataset built\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0da36896",
   "metadata": {},
   "outputs": [],
   "source": [
    "ledger_df = ledger.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7bf0494",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['market_id', 'block_number', 'log_index', 'block_timestamp', 'tx_hash',\n",
       "       'event_type', 'delta_supply_assets', 'delta_borrow_assets',\n",
       "       'delta_collateral_assets', 'total_supplied_assets',\n",
       "       'outstanding_borrow_assets', 'total_collateral_assets',\n",
       "       'borrow_rate_per_sec', 'utilization_rate', 'borrow_apy',\n",
       "       'supply_rate_per_sec', 'supply_apy', 'delta_utilization',\n",
       "       'delta_borrow_apy', 'delta_supply_apy', 'borrow_apr',\n",
       "       'borrow_in_assets', 'borrow_out_assets', 'supply_in_assets',\n",
       "       'supply_out_assets', 'interest_assets', 'util_mean_5min',\n",
       "       'util_std_5min', 'util_mean_1h', 'util_std_1h', 'util_mean_6h',\n",
       "       'util_std_6h', 'borrow_apr_mean_5min', 'borrow_apr_std_5min',\n",
       "       'borrow_apr_mean_1h', 'borrow_apr_std_1h', 'borrow_apr_mean_6h',\n",
       "       'borrow_apr_std_6h', 'borrow_intensity_5min', 'repay_intensity_5min',\n",
       "       'supply_intensity_5min', 'withdraw_intensity_5min',\n",
       "       'interest_intensity_5min', 'borrow_intensity_1h', 'repay_intensity_1h',\n",
       "       'supply_intensity_1h', 'withdraw_intensity_1h', 'interest_intensity_1h',\n",
       "       'borrow_intensity_6h', 'repay_intensity_6h', 'supply_intensity_6h',\n",
       "       'withdraw_intensity_6h', 'interest_intensity_6h', 'delta_borrow_apr',\n",
       "       'elasticity_apr_per_util', 'regime_util_gt_90', 'regime_util_gt_95',\n",
       "       'regime_util_gt_99', 'borrow_apr_6h_avg_ui'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ledger_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885dcaaa",
   "metadata": {},
   "source": [
    "## NEW: Build Tick-Aggregated Ledger\n",
    "\n",
    "Major performance optimization! Instead of working with event-level data, we can aggregate to tick-level (e.g., 5min windows) or block-level.\n",
    "\n",
    "**Benefits:**\n",
    "- **90-99% data reduction**: 10,000 events → 100 ticks\n",
    "- **Much faster rolling windows**: Operating on 100 rows vs 10,000\n",
    "- **Lower memory usage**: Smaller dataset = less RAM\n",
    "- **Preserves attribution accuracy**: Flows are aggregated correctly\n",
    "\n",
    "**How it works:**\n",
    "1. Compute cumulative states at event-level (precise)\n",
    "2. Group events into ticks/blocks\n",
    "3. Sum deltas, take final states\n",
    "4. Aggregate flows by type (supply_in, borrow_in, etc.)\n",
    "5. Fill empty ticks with zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abca4518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tick-aggregated ledger...\n"
     ]
    }
   ],
   "source": [
    "# Build tick-aggregated ledger (5-minute ticks)\n",
    "print(\"Building tick-aggregated ledger...\")\n",
    "tick_ledger = service.build_tick_ledger(\n",
    "    market_id=CBBTC_MARKET_ID,\n",
    "    tick_seconds=300,  # 5 minutes\n",
    "    aggregation_level=\"tick\",\n",
    ")\n",
    "\n",
    "tick_df = tick_ledger.table.to_pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f9f6545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>market_id</th>\n",
       "      <th>block_number</th>\n",
       "      <th>log_index</th>\n",
       "      <th>block_timestamp</th>\n",
       "      <th>tx_hash</th>\n",
       "      <th>event_type</th>\n",
       "      <th>delta_supply_assets</th>\n",
       "      <th>delta_borrow_assets</th>\n",
       "      <th>delta_collateral_assets</th>\n",
       "      <th>total_supplied_assets</th>\n",
       "      <th>...</th>\n",
       "      <th>borrow_in_assets</th>\n",
       "      <th>repay_out_assets</th>\n",
       "      <th>liquidate_repay_assets</th>\n",
       "      <th>interest_assets</th>\n",
       "      <th>collateral_in_assets</th>\n",
       "      <th>collateral_out_assets</th>\n",
       "      <th>event_count</th>\n",
       "      <th>delta_utilization</th>\n",
       "      <th>delta_borrow_apy</th>\n",
       "      <th>delta_supply_apy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x9103c3b4e834476c9a62ea009ba2c884ee42e94e6e31...</td>\n",
       "      <td>19403244</td>\n",
       "      <td>265</td>\n",
       "      <td>1725595835</td>\n",
       "      <td>0x808559ab9f6195db59cac4cb8205cbfd617ca6eb7013...</td>\n",
       "      <td>Aggregated</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x9103c3b4e834476c9a62ea009ba2c884ee42e94e6e31...</td>\n",
       "      <td>19403244</td>\n",
       "      <td>265</td>\n",
       "      <td>1725596400</td>\n",
       "      <td>0x808559ab9f6195db59cac4cb8205cbfd617ca6eb7013...</td>\n",
       "      <td>Aggregated</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x9103c3b4e834476c9a62ea009ba2c884ee42e94e6e31...</td>\n",
       "      <td>19403244</td>\n",
       "      <td>265</td>\n",
       "      <td>1725596700</td>\n",
       "      <td>0x808559ab9f6195db59cac4cb8205cbfd617ca6eb7013...</td>\n",
       "      <td>Aggregated</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x9103c3b4e834476c9a62ea009ba2c884ee42e94e6e31...</td>\n",
       "      <td>19403244</td>\n",
       "      <td>265</td>\n",
       "      <td>1725597000</td>\n",
       "      <td>0x808559ab9f6195db59cac4cb8205cbfd617ca6eb7013...</td>\n",
       "      <td>Aggregated</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x9103c3b4e834476c9a62ea009ba2c884ee42e94e6e31...</td>\n",
       "      <td>19403244</td>\n",
       "      <td>265</td>\n",
       "      <td>1725597300</td>\n",
       "      <td>0x808559ab9f6195db59cac4cb8205cbfd617ca6eb7013...</td>\n",
       "      <td>Aggregated</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136356</th>\n",
       "      <td>0x9103c3b4e834476c9a62ea009ba2c884ee42e94e6e31...</td>\n",
       "      <td>39856769</td>\n",
       "      <td>406</td>\n",
       "      <td>1766502885</td>\n",
       "      <td>0x49a0cf9ef5f6b788d6a926cfafbf00513dad4a8fd3aa...</td>\n",
       "      <td>Aggregated</td>\n",
       "      <td>-18919184428</td>\n",
       "      <td>7854835952</td>\n",
       "      <td>5960437</td>\n",
       "      <td>1140933652721324</td>\n",
       "      <td>...</td>\n",
       "      <td>27342470000</td>\n",
       "      <td>20031834989</td>\n",
       "      <td>0</td>\n",
       "      <td>544200941</td>\n",
       "      <td>80437681</td>\n",
       "      <td>74477244</td>\n",
       "      <td>131</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>6.075401e-07</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136357</th>\n",
       "      <td>0x9103c3b4e834476c9a62ea009ba2c884ee42e94e6e31...</td>\n",
       "      <td>39856920</td>\n",
       "      <td>276</td>\n",
       "      <td>1766503187</td>\n",
       "      <td>0x50aa4931a5157dbbecb4ff48d12d724dc8c4dc6f1175...</td>\n",
       "      <td>Aggregated</td>\n",
       "      <td>53473063445</td>\n",
       "      <td>8339538697</td>\n",
       "      <td>12394972</td>\n",
       "      <td>1140987125784769</td>\n",
       "      <td>...</td>\n",
       "      <td>8265000000</td>\n",
       "      <td>500000000</td>\n",
       "      <td>0</td>\n",
       "      <td>574538697</td>\n",
       "      <td>12394972</td>\n",
       "      <td>0</td>\n",
       "      <td>170</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>-2.458677e-06</td>\n",
       "      <td>-0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136358</th>\n",
       "      <td>0x9103c3b4e834476c9a62ea009ba2c884ee42e94e6e31...</td>\n",
       "      <td>39857068</td>\n",
       "      <td>471</td>\n",
       "      <td>1766503483</td>\n",
       "      <td>0x65b17c07903665733608c2edc02d505c68d992ae143b...</td>\n",
       "      <td>Aggregated</td>\n",
       "      <td>553786621465</td>\n",
       "      <td>28524842387</td>\n",
       "      <td>-436583</td>\n",
       "      <td>1141540912406234</td>\n",
       "      <td>...</td>\n",
       "      <td>32462260000</td>\n",
       "      <td>4500624684</td>\n",
       "      <td>0</td>\n",
       "      <td>563207071</td>\n",
       "      <td>11118200</td>\n",
       "      <td>11554783</td>\n",
       "      <td>158</td>\n",
       "      <td>-0.000403</td>\n",
       "      <td>-2.216101e-05</td>\n",
       "      <td>-0.000045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136359</th>\n",
       "      <td>0x9103c3b4e834476c9a62ea009ba2c884ee42e94e6e31...</td>\n",
       "      <td>39857225</td>\n",
       "      <td>1459</td>\n",
       "      <td>1766503797</td>\n",
       "      <td>0x47f7e45ac2093f3383751f2148705981cae502d3ab92...</td>\n",
       "      <td>Aggregated</td>\n",
       "      <td>-500011791263</td>\n",
       "      <td>30447460877</td>\n",
       "      <td>1664802</td>\n",
       "      <td>1141040900614971</td>\n",
       "      <td>...</td>\n",
       "      <td>31500000000</td>\n",
       "      <td>1650000000</td>\n",
       "      <td>0</td>\n",
       "      <td>597460877</td>\n",
       "      <td>1664802</td>\n",
       "      <td>0</td>\n",
       "      <td>113</td>\n",
       "      <td>0.000413</td>\n",
       "      <td>2.190270e-05</td>\n",
       "      <td>0.000045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136360</th>\n",
       "      <td>0x9103c3b4e834476c9a62ea009ba2c884ee42e94e6e31...</td>\n",
       "      <td>39857315</td>\n",
       "      <td>100</td>\n",
       "      <td>1766503977</td>\n",
       "      <td>0xe27432cc2af3280ef4a9403fd63c6a359ac146e5df86...</td>\n",
       "      <td>Aggregated</td>\n",
       "      <td>-23468790045</td>\n",
       "      <td>547518628</td>\n",
       "      <td>5464058</td>\n",
       "      <td>1141017431824926</td>\n",
       "      <td>...</td>\n",
       "      <td>505000000</td>\n",
       "      <td>300000000</td>\n",
       "      <td>0</td>\n",
       "      <td>342518628</td>\n",
       "      <td>5464058</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>2.236680e-07</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>136361 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                market_id  block_number  \\\n",
       "0       0x9103c3b4e834476c9a62ea009ba2c884ee42e94e6e31...      19403244   \n",
       "1       0x9103c3b4e834476c9a62ea009ba2c884ee42e94e6e31...      19403244   \n",
       "2       0x9103c3b4e834476c9a62ea009ba2c884ee42e94e6e31...      19403244   \n",
       "3       0x9103c3b4e834476c9a62ea009ba2c884ee42e94e6e31...      19403244   \n",
       "4       0x9103c3b4e834476c9a62ea009ba2c884ee42e94e6e31...      19403244   \n",
       "...                                                   ...           ...   \n",
       "136356  0x9103c3b4e834476c9a62ea009ba2c884ee42e94e6e31...      39856769   \n",
       "136357  0x9103c3b4e834476c9a62ea009ba2c884ee42e94e6e31...      39856920   \n",
       "136358  0x9103c3b4e834476c9a62ea009ba2c884ee42e94e6e31...      39857068   \n",
       "136359  0x9103c3b4e834476c9a62ea009ba2c884ee42e94e6e31...      39857225   \n",
       "136360  0x9103c3b4e834476c9a62ea009ba2c884ee42e94e6e31...      39857315   \n",
       "\n",
       "        log_index  block_timestamp  \\\n",
       "0             265       1725595835   \n",
       "1             265       1725596400   \n",
       "2             265       1725596700   \n",
       "3             265       1725597000   \n",
       "4             265       1725597300   \n",
       "...           ...              ...   \n",
       "136356        406       1766502885   \n",
       "136357        276       1766503187   \n",
       "136358        471       1766503483   \n",
       "136359       1459       1766503797   \n",
       "136360        100       1766503977   \n",
       "\n",
       "                                                  tx_hash  event_type  \\\n",
       "0       0x808559ab9f6195db59cac4cb8205cbfd617ca6eb7013...  Aggregated   \n",
       "1       0x808559ab9f6195db59cac4cb8205cbfd617ca6eb7013...  Aggregated   \n",
       "2       0x808559ab9f6195db59cac4cb8205cbfd617ca6eb7013...  Aggregated   \n",
       "3       0x808559ab9f6195db59cac4cb8205cbfd617ca6eb7013...  Aggregated   \n",
       "4       0x808559ab9f6195db59cac4cb8205cbfd617ca6eb7013...  Aggregated   \n",
       "...                                                   ...         ...   \n",
       "136356  0x49a0cf9ef5f6b788d6a926cfafbf00513dad4a8fd3aa...  Aggregated   \n",
       "136357  0x50aa4931a5157dbbecb4ff48d12d724dc8c4dc6f1175...  Aggregated   \n",
       "136358  0x65b17c07903665733608c2edc02d505c68d992ae143b...  Aggregated   \n",
       "136359  0x47f7e45ac2093f3383751f2148705981cae502d3ab92...  Aggregated   \n",
       "136360  0xe27432cc2af3280ef4a9403fd63c6a359ac146e5df86...  Aggregated   \n",
       "\n",
       "       delta_supply_assets delta_borrow_assets delta_collateral_assets  \\\n",
       "0                        0                   0                       0   \n",
       "1                        0                   0                       0   \n",
       "2                        0                   0                       0   \n",
       "3                        0                   0                       0   \n",
       "4                        0                   0                       0   \n",
       "...                    ...                 ...                     ...   \n",
       "136356        -18919184428          7854835952                 5960437   \n",
       "136357         53473063445          8339538697                12394972   \n",
       "136358        553786621465         28524842387                 -436583   \n",
       "136359       -500011791263         30447460877                 1664802   \n",
       "136360        -23468790045           547518628                 5464058   \n",
       "\n",
       "        total_supplied_assets  ...  borrow_in_assets  repay_out_assets  \\\n",
       "0                           0  ...                 0                 0   \n",
       "1                           0  ...                 0                 0   \n",
       "2                           0  ...                 0                 0   \n",
       "3                           0  ...                 0                 0   \n",
       "4                           0  ...                 0                 0   \n",
       "...                       ...  ...               ...               ...   \n",
       "136356       1140933652721324  ...       27342470000       20031834989   \n",
       "136357       1140987125784769  ...        8265000000         500000000   \n",
       "136358       1141540912406234  ...       32462260000        4500624684   \n",
       "136359       1141040900614971  ...       31500000000        1650000000   \n",
       "136360       1141017431824926  ...         505000000         300000000   \n",
       "\n",
       "        liquidate_repay_assets  interest_assets  collateral_in_assets  \\\n",
       "0                            0                0                     0   \n",
       "1                            0                0                     0   \n",
       "2                            0                0                     0   \n",
       "3                            0                0                     0   \n",
       "4                            0                0                     0   \n",
       "...                        ...              ...                   ...   \n",
       "136356                       0        544200941              80437681   \n",
       "136357                       0        574538697              12394972   \n",
       "136358                       0        563207071              11118200   \n",
       "136359                       0        597460877               1664802   \n",
       "136360                       0        342518628               5464058   \n",
       "\n",
       "        collateral_out_assets  event_count delta_utilization delta_borrow_apy  \\\n",
       "0                           0            1          0.000000     0.000000e+00   \n",
       "1                           0            0          0.000000     0.000000e+00   \n",
       "2                           0            0          0.000000     0.000000e+00   \n",
       "3                           0            0          0.000000     0.000000e+00   \n",
       "4                           0            0          0.000000     0.000000e+00   \n",
       "...                       ...          ...               ...              ...   \n",
       "136356               74477244          131          0.000022     6.075401e-07   \n",
       "136357                      0          170         -0.000034    -2.458677e-06   \n",
       "136358               11554783          158         -0.000403    -2.216101e-05   \n",
       "136359                      0          113          0.000413     2.190270e-05   \n",
       "136360                      0           64          0.000019     2.236680e-07   \n",
       "\n",
       "       delta_supply_apy  \n",
       "0              0.000000  \n",
       "1              0.000000  \n",
       "2              0.000000  \n",
       "3              0.000000  \n",
       "4              0.000000  \n",
       "...                 ...  \n",
       "136356         0.000002  \n",
       "136357        -0.000004  \n",
       "136358        -0.000045  \n",
       "136359         0.000045  \n",
       "136360         0.000001  \n",
       "\n",
       "[136361 rows x 29 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tick_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "886616ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ACCOUNTING INTEGRITY CHECK\n",
      "============================================================\n",
      "Borrow side:\n",
      "  Flow sum: 0\n",
      "  Delta:    0\n",
      "  Match: ✓\n",
      "\n",
      "Supply side:\n",
      "  Flow sum: 1,000,000\n",
      "  Delta:    1,000,000\n",
      "  Match: ✓\n"
     ]
    }
   ],
   "source": [
    "# Verify accounting integrity: sum of flow deltas should equal total delta\n",
    "sample_tick = tick_df[tick_df['event_count'] > 1].iloc[0] if (tick_df['event_count'] > 1).any() else tick_df.iloc[10]\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"ACCOUNTING INTEGRITY CHECK\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Borrow side\n",
    "borrow_flows_sum = (sample_tick['borrow_in_assets'] - \n",
    "                    sample_tick['repay_out_assets'] - \n",
    "                    sample_tick['liquidate_repay_assets'] + \n",
    "                    sample_tick['interest_assets'])\n",
    "borrow_delta = sample_tick['delta_borrow_assets']\n",
    "print(f\"Borrow side:\")\n",
    "print(f\"  Flow sum: {borrow_flows_sum:,.0f}\")\n",
    "print(f\"  Delta:    {borrow_delta:,.0f}\")\n",
    "print(f\"  Match: {'✓' if abs(borrow_flows_sum - borrow_delta) < 1 else '✗'}\")\n",
    "\n",
    "# Supply side\n",
    "supply_flows_sum = (sample_tick['supply_in_assets'] - \n",
    "                    sample_tick['withdraw_out_assets'] +\n",
    "                    sample_tick['interest_assets'])\n",
    "supply_delta = sample_tick['delta_supply_assets']\n",
    "print(f\"\\nSupply side:\")\n",
    "print(f\"  Flow sum: {supply_flows_sum:,.0f}\")\n",
    "print(f\"  Delta:    {supply_delta:,.0f}\")\n",
    "print(f\"  Match: {'✓' if abs(supply_flows_sum - supply_delta) < 1 else '✗'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4723d208",
   "metadata": {},
   "source": [
    "## Build Attribution Feature Dataset with Optimizations\n",
    "\n",
    "The attribution pipeline follows a two-step process:\n",
    "1. Build the enriched market ledger (raw events → standardized → enriched with state)\n",
    "2. Compute attribution features from the ledger\n",
    "\n",
    "**NEW: Data Preprocessing & Batch Processing**\n",
    "\n",
    "We've added two major optimizations to reduce memory usage and processing time:\n",
    "\n",
    "1. **Tick-based preprocessing** (`preprocess_ticks=True`): \n",
    "   - For 5-minute window analysis, keeps only first and last rows within each tick\n",
    "   - Can reduce dataset size by 90%+ for high-frequency event data\n",
    "   - Preserves accuracy for rolling window calculations\n",
    "\n",
    "2. **Batch processing** (`batch_size`):\n",
    "   - Processes rolling windows in batches to reduce memory footprint\n",
    "   - Useful when computing many windows (e.g., all 8 windows)\n",
    "\n",
    "3. **Window selection** (`compute_windows`):\n",
    "   - Specify only the windows you need (e.g., ['5min', '1h', '24h'])\n",
    "   - Each window adds significant memory overhead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c434f67b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing: Reduced from 4,084,573 to 222,839 rows (94.5% reduction)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/youssef/morpho_blue/src/morpho_blue/transformations/attribution.py:364: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[f\"interest_intensity_{window_label}\"] = df_batch[\"interest_assets\"].rolling(window_label, min_periods=1).sum() / seconds_w\n",
      "/home/youssef/morpho_blue/src/morpho_blue/transformations/attribution.py:365: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \n",
      "/home/youssef/morpho_blue/src/morpho_blue/transformations/attribution.py:366: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  # Scale-free intensity (normalized by mean supply)\n",
      "/home/youssef/morpho_blue/src/morpho_blue/transformations/attribution.py:367: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mean_supply_window = df_batch[\"total_supplied_assets\"].astype(\"float64\").rolling(window_label, min_periods=1).mean()\n",
      "/home/youssef/morpho_blue/src/morpho_blue/transformations/attribution.py:373: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[f\"withdraw_intensity_norm_supply_{window_label}\"] = (\n",
      "/home/youssef/morpho_blue/src/morpho_blue/transformations/attribution.py:376: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[f\"repay_intensity_norm_supply_{window_label}\"] = (\n",
      "/home/youssef/morpho_blue/src/morpho_blue/transformations/attribution.py:379: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \n",
      "/home/youssef/morpho_blue/src/morpho_blue/transformations/attribution.py:384: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[f\"contrib_borrow_sum_{window_label}\"] = df_batch[\"contrib_u_from_borrow\"].rolling(window_label, min_periods=1).sum()\n",
      "/home/youssef/morpho_blue/src/morpho_blue/transformations/attribution.py:385: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[f\"contrib_supply_sum_{window_label}\"] = df_batch[\"contrib_u_from_supply\"].rolling(window_label, min_periods=1).sum()\n",
      "/home/youssef/morpho_blue/src/morpho_blue/transformations/attribution.py:386: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \n",
      "/home/youssef/morpho_blue/src/morpho_blue/transformations/attribution.py:387: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  # Absolute contribution sums (activity magnitude)\n",
      "/home/youssef/morpho_blue/src/morpho_blue/transformations/attribution.py:388: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[f\"contrib_withdraw_abs_sum_{window_label}\"] = df_batch[\"contrib_u_from_withdraw\"].abs().rolling(window_label, min_periods=1).sum()\n",
      "/home/youssef/morpho_blue/src/morpho_blue/transformations/attribution.py:391: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \n",
      "/home/youssef/morpho_blue/src/morpho_blue/transformations/attribution.py:392: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  # Volatility / fragility\n",
      "/home/youssef/morpho_blue/src/morpho_blue/transformations/attribution.py:393: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_batch[f\"delta_u_std_{window_label}\"] = df_batch[\"delta_u_real\"].rolling(window_label, min_periods=2).std().fillna(0.0)\n",
      "/home/youssef/morpho_blue/src/morpho_blue/transformations/attribution.py:396: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  irm_slope_winsorized = df_batch[\"irm_slope\"].clip(lower=-1000, upper=1000).fillna(0.0)\n",
      "/home/youssef/morpho_blue/src/morpho_blue/transformations/attribution.py:400: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \n",
      "/home/youssef/morpho_blue/src/morpho_blue/transformations/attribution.py:433: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  # F) EXPANDING WINDOWS (ALL-TIME STATISTICS)\n",
      "/home/youssef/morpho_blue/src/morpho_blue/transformations/attribution.py:433: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  # F) EXPANDING WINDOWS (ALL-TIME STATISTICS)\n",
      "/home/youssef/morpho_blue/src/morpho_blue/transformations/attribution.py:433: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  # F) EXPANDING WINDOWS (ALL-TIME STATISTICS)\n",
      "/home/youssef/morpho_blue/src/morpho_blue/transformations/attribution.py:433: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  # F) EXPANDING WINDOWS (ALL-TIME STATISTICS)\n",
      "/home/youssef/morpho_blue/src/morpho_blue/transformations/attribution.py:433: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  # F) EXPANDING WINDOWS (ALL-TIME STATISTICS)\n",
      "/home/youssef/morpho_blue/src/morpho_blue/transformations/attribution.py:433: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  # F) EXPANDING WINDOWS (ALL-TIME STATISTICS)\n",
      "/home/youssef/morpho_blue/src/morpho_blue/transformations/attribution.py:433: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  # F) EXPANDING WINDOWS (ALL-TIME STATISTICS)\n",
      "/home/youssef/morpho_blue/src/morpho_blue/transformations/attribution.py:433: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  # F) EXPANDING WINDOWS (ALL-TIME STATISTICS)\n",
      "/home/youssef/morpho_blue/src/morpho_blue/transformations/attribution.py:433: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  # F) EXPANDING WINDOWS (ALL-TIME STATISTICS)\n",
      "/home/youssef/morpho_blue/src/morpho_blue/transformations/attribution.py:433: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  # F) EXPANDING WINDOWS (ALL-TIME STATISTICS)\n",
      "/home/youssef/morpho_blue/src/morpho_blue/transformations/attribution.py:433: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  # F) EXPANDING WINDOWS (ALL-TIME STATISTICS)\n",
      "/home/youssef/morpho_blue/src/morpho_blue/transformations/attribution.py:433: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  # F) EXPANDING WINDOWS (ALL-TIME STATISTICS)\n",
      "/home/youssef/morpho_blue/src/morpho_blue/transformations/attribution.py:433: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  # F) EXPANDING WINDOWS (ALL-TIME STATISTICS)\n",
      "/home/youssef/morpho_blue/src/morpho_blue/transformations/attribution.py:433: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  # F) EXPANDING WINDOWS (ALL-TIME STATISTICS)\n",
      "/home/youssef/morpho_blue/src/morpho_blue/transformations/attribution.py:433: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  # F) EXPANDING WINDOWS (ALL-TIME STATISTICS)\n",
      "/home/youssef/morpho_blue/src/morpho_blue/transformations/attribution.py:439: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \n",
      "/home/youssef/morpho_blue/src/morpho_blue/transformations/attribution.py:440: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"borrow_apr_mean_expanding\"] = df[\"borrow_apr\"].expanding(min_periods=1).mean()\n",
      "/home/youssef/morpho_blue/src/morpho_blue/transformations/attribution.py:441: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"borrow_apr_std_expanding\"] = df[\"borrow_apr\"].expanding(min_periods=2).std().fillna(0.0)\n",
      "/home/youssef/morpho_blue/src/morpho_blue/transformations/attribution.py:443: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \n",
      "/home/youssef/morpho_blue/src/morpho_blue/transformations/attribution.py:444: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"delta_u_std_expanding\"] = df[\"delta_u_real\"].expanding(min_periods=2).std().fillna(0.0)\n",
      "/home/youssef/morpho_blue/src/morpho_blue/transformations/attribution.py:445: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \n",
      "/home/youssef/morpho_blue/src/morpho_blue/transformations/attribution.py:447: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  # RESET INDEX AND CONVERT TO ARROW\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Attribution dataset built\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "attribution = service.build_attribution_dataset(\n",
    "    ledger=ledger,\n",
    "    compute_windows=['5min', '1h', '24h'],  # Only compute these 3 windows\n",
    "    validate=True,\n",
    "    preprocess_ticks=True,  # NEW: Reduce data by keeping only first/last rows per tick\n",
    "    tick_seconds=300,       # 5-minute ticks\n",
    "    batch_size=None,        # Set to 2 or 3 to batch process windows\n",
    ")\n",
    "\n",
    "df = attribution.table.to_pandas()\n",
    "print(f\"\\n✓ Attribution dataset built\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1b70546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['market_id', 'block_number', 'log_index', 'block_timestamp', 'tx_hash',\n",
      "       'event_type', 'delta_supply_assets', 'delta_borrow_assets',\n",
      "       'delta_collateral_assets', 'total_supplied_assets',\n",
      "       'outstanding_borrow_assets', 'total_collateral_assets',\n",
      "       'borrow_rate_per_sec', 'utilization_rate', 'borrow_apy',\n",
      "       'supply_rate_per_sec', 'supply_apy', 'delta_utilization',\n",
      "       'delta_borrow_apy', 'delta_supply_apy', 'borrow_apr',\n",
      "       'borrow_in_assets', 'borrow_out_assets', 'supply_in_assets',\n",
      "       'supply_out_assets', 'interest_assets', 'util_mean_5min',\n",
      "       'util_std_5min', 'util_mean_1h', 'util_std_1h', 'util_mean_6h',\n",
      "       'util_std_6h', 'borrow_apr_mean_5min', 'borrow_apr_std_5min',\n",
      "       'borrow_apr_mean_1h', 'borrow_apr_std_1h', 'borrow_apr_mean_6h',\n",
      "       'borrow_apr_std_6h', 'borrow_intensity_5min', 'repay_intensity_5min',\n",
      "       'supply_intensity_5min', 'withdraw_intensity_5min',\n",
      "       'interest_intensity_5min', 'borrow_intensity_1h', 'repay_intensity_1h',\n",
      "       'supply_intensity_1h', 'withdraw_intensity_1h', 'interest_intensity_1h',\n",
      "       'borrow_intensity_6h', 'repay_intensity_6h'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817d1fff",
   "metadata": {},
   "source": [
    "## Inspect Core Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0bf99468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Columns Sample:\n",
      "   block_number        event_type  utilization_rate    buffer  borrow_apr  \\\n",
      "0      19403244    AccrueInterest          0.000000  1.000000    0.008901   \n",
      "1      19713959    AccrueInterest          0.000000  1.000000    0.005110   \n",
      "2      19713959            Supply          0.000000  1.000000    0.005110   \n",
      "3      19714179  SupplyCollateral          0.000000  1.000000    0.005110   \n",
      "4      19714179            Borrow          0.983789  0.016211    0.002954   \n",
      "5      19726214  SupplyCollateral          0.983789  0.016211    0.002954   \n",
      "6      19726214            Borrow          0.980745  0.019255    0.042173   \n",
      "7      19727323  SupplyCollateral          0.980745  0.019255    0.042173   \n",
      "8      19727323            Borrow          0.980566  0.019434    0.041796   \n",
      "9      19729336  SupplyCollateral          0.980566  0.019434    0.041796   \n",
      "\n",
      "   headroom_u90_assets  headroom_u95_assets  \n",
      "0                    0                    0  \n",
      "1                    0                    0  \n",
      "2               900000               950000  \n",
      "3               900000               950000  \n",
      "4               -85169               -34345  \n",
      "5               -85169               -34345  \n",
      "6         -12349690292          -4702395306  \n",
      "7         -12349690292          -4702395306  \n",
      "8         -12336883371          -4680519060  \n",
      "9         -12336883371          -4680519060  \n",
      "\n",
      "Utilization range: [0.0000, 1.0000]\n",
      "Borrow APR range: [0.0030, 0.2997]\n"
     ]
    }
   ],
   "source": [
    "# Show sample of state columns\n",
    "state_cols = [\n",
    "    'block_number', 'event_type', 'utilization_rate', 'buffer',\n",
    "    'borrow_apr', 'headroom_u90_assets', 'headroom_u95_assets'\n",
    "]\n",
    "\n",
    "print(\"State Columns Sample:\")\n",
    "print(df[state_cols].head(10))\n",
    "print(f\"\\nUtilization range: [{df['utilization_rate'].min():.4f}, {df['utilization_rate'].max():.4f}]\")\n",
    "print(f\"Borrow APR range: [{df['borrow_apr'].min():.4f}, {df['borrow_apr'].max():.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "57d79d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Flow Decomposition (non-zero rows):\n",
      "        event_type  borrow_in_assets  repay_out_assets  \\\n",
      "2           Supply      0.000000e+00               0.0   \n",
      "4           Borrow      1.000000e+06               0.0   \n",
      "6           Borrow      1.500000e+11               0.0   \n",
      "8           Borrow      1.500000e+08               0.0   \n",
      "10          Borrow      2.500000e+08               0.0   \n",
      "11  AccrueInterest      0.000000e+00               0.0   \n",
      "12          Supply      0.000000e+00               0.0   \n",
      "13  AccrueInterest      0.000000e+00               0.0   \n",
      "14        Withdraw      0.000000e+00               0.0   \n",
      "15  AccrueInterest      0.000000e+00               0.0   \n",
      "16          Supply      0.000000e+00               0.0   \n",
      "17  AccrueInterest      0.000000e+00               0.0   \n",
      "18        Withdraw      0.000000e+00               0.0   \n",
      "20          Borrow      1.765100e+10               0.0   \n",
      "21  AccrueInterest      0.000000e+00               0.0   \n",
      "22          Supply      0.000000e+00               0.0   \n",
      "23  AccrueInterest      0.000000e+00               0.0   \n",
      "24        Withdraw      0.000000e+00               0.0   \n",
      "25  AccrueInterest      0.000000e+00               0.0   \n",
      "26          Supply      0.000000e+00               0.0   \n",
      "\n",
      "    liquidate_repay_assets  supply_in_assets  withdraw_out_assets  \\\n",
      "2                      0.0      1.000000e+06         0.000000e+00   \n",
      "4                      0.0      0.000000e+00         0.000000e+00   \n",
      "6                      0.0      0.000000e+00         0.000000e+00   \n",
      "8                      0.0      0.000000e+00         0.000000e+00   \n",
      "10                     0.0      0.000000e+00         0.000000e+00   \n",
      "11                     0.0      0.000000e+00         0.000000e+00   \n",
      "12                     0.0      8.050486e+09         0.000000e+00   \n",
      "13                     0.0      0.000000e+00         0.000000e+00   \n",
      "14                     0.0      0.000000e+00         6.790846e+09   \n",
      "15                     0.0      0.000000e+00         0.000000e+00   \n",
      "16                     0.0      6.901800e+09         0.000000e+00   \n",
      "17                     0.0      0.000000e+00         0.000000e+00   \n",
      "18                     0.0      0.000000e+00         6.536226e+09   \n",
      "20                     0.0      0.000000e+00         0.000000e+00   \n",
      "21                     0.0      0.000000e+00         0.000000e+00   \n",
      "22                     0.0      6.565464e+09         0.000000e+00   \n",
      "23                     0.0      0.000000e+00         0.000000e+00   \n",
      "24                     0.0      0.000000e+00         6.384803e+09   \n",
      "25                     0.0      0.000000e+00         0.000000e+00   \n",
      "26                     0.0      6.144943e+09         0.000000e+00   \n",
      "\n",
      "    interest_assets  \n",
      "2               0.0  \n",
      "4               0.0  \n",
      "6               0.0  \n",
      "8               0.0  \n",
      "10              0.0  \n",
      "11        2694864.0  \n",
      "12              0.0  \n",
      "13          29191.0  \n",
      "14              0.0  \n",
      "15         297643.0  \n",
      "16              0.0  \n",
      "17          29671.0  \n",
      "18              0.0  \n",
      "20              0.0  \n",
      "21         363590.0  \n",
      "22              0.0  \n",
      "23          35996.0  \n",
      "24              0.0  \n",
      "25         327415.0  \n",
      "26              0.0  \n",
      "\n",
      "Flow totals:\n",
      "  Total borrowed: 240,615,893,365,216\n",
      "  Total repaid: 40,456,742,682,598\n",
      "  Total liquidated: 917,029,684,309\n",
      "  Total supplied: 1,945,293,766,572,757\n",
      "  Total withdrawn: 1,411,537,847,187,706\n",
      "  Total interest: 4,290,729,904,748\n"
     ]
    }
   ],
   "source": [
    "# Show flow decomposition\n",
    "flow_cols = [\n",
    "    'event_type', 'borrow_in_assets', 'repay_out_assets', 'liquidate_repay_assets',\n",
    "    'supply_in_assets', 'withdraw_out_assets', 'interest_assets'\n",
    "]\n",
    "\n",
    "print(\"\\nFlow Decomposition (non-zero rows):\")\n",
    "flow_df = df[flow_cols]\n",
    "non_zero = flow_df[(flow_df.iloc[:, 1:] != 0).any(axis=1)]\n",
    "print(non_zero.head(20))\n",
    "\n",
    "print(\"\\nFlow totals:\")\n",
    "print(f\"  Total borrowed: {df['borrow_in_assets'].sum():,.0f}\")\n",
    "print(f\"  Total repaid: {df['repay_out_assets'].sum():,.0f}\")\n",
    "print(f\"  Total liquidated: {df['liquidate_repay_assets'].sum():,.0f}\")\n",
    "print(f\"  Total supplied: {df['supply_in_assets'].sum():,.0f}\")\n",
    "print(f\"  Total withdrawn: {df['withdraw_out_assets'].sum():,.0f}\")\n",
    "print(f\"  Total interest: {df['interest_assets'].sum():,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3ecc77d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Utilization Contribution Terms (rows with significant change):\n",
      "   event_type  delta_u_real   delta_u_pred  contrib_u_from_borrow  \\\n",
      "4      Borrow      0.983789       1.000000               1.000000   \n",
      "6      Borrow     -0.003044  147568.368425          147568.368425   \n",
      "10     Borrow      0.001450       0.001633               0.001633   \n",
      "12     Supply     -0.049040      -0.051618               0.000000   \n",
      "14   Withdraw      0.041029       0.039301               0.000000   \n",
      "16     Supply     -0.041671      -0.043533               0.000000   \n",
      "18   Withdraw      0.039370       0.037775               0.000000   \n",
      "20     Borrow      0.001030       0.114036               0.114036   \n",
      "22     Supply     -0.035612      -0.036966               0.000000   \n",
      "24   Withdraw      0.034596       0.033364               0.000000   \n",
      "26     Supply     -0.033341      -0.034526               0.000000   \n",
      "28     Borrow      0.033205       0.000223               0.000223   \n",
      "31     Borrow     -0.001759       0.000202               0.000202   \n",
      "38     Supply     -0.045614      -0.047865               0.000000   \n",
      "40      Repay     -0.771342      -0.778056               0.000000   \n",
      "42   Withdraw      0.613093       0.122317               0.000000   \n",
      "46     Borrow      0.198267       1.359684               1.359684   \n",
      "48   Withdraw     -0.198267       0.526777               0.000000   \n",
      "50   Withdraw      0.172297       0.140650               0.000000   \n",
      "52   Withdraw      0.022546       0.022017               0.000000   \n",
      "\n",
      "    contrib_u_from_repay  contrib_u_from_liquidate  contrib_u_from_withdraw  \\\n",
      "4              -0.000000                      -0.0                 0.000000   \n",
      "6              -0.000000                      -0.0                 0.000000   \n",
      "10             -0.000000                      -0.0                 0.000000   \n",
      "12             -0.000000                      -0.0                 0.000000   \n",
      "14             -0.000000                      -0.0                 0.039301   \n",
      "16             -0.000000                      -0.0                 0.000000   \n",
      "18             -0.000000                      -0.0                 0.037775   \n",
      "20             -0.000000                      -0.0                 0.000000   \n",
      "22             -0.000000                      -0.0                 0.000000   \n",
      "24             -0.000000                      -0.0                 0.033364   \n",
      "26             -0.000000                      -0.0                 0.000000   \n",
      "28             -0.000000                      -0.0                 0.000000   \n",
      "31             -0.000000                      -0.0                 0.000000   \n",
      "38             -0.000000                      -0.0                 0.000000   \n",
      "40             -0.778056                      -0.0                 0.000000   \n",
      "42             -0.000000                      -0.0                 0.122317   \n",
      "46             -0.000000                      -0.0                 0.000000   \n",
      "48             -0.000000                      -0.0                 0.526777   \n",
      "50             -0.000000                      -0.0                 0.140650   \n",
      "52             -0.000000                      -0.0                 0.022017   \n",
      "\n",
      "    contrib_u_from_supply  contrib_u_from_interest  delta_u_residual  \n",
      "4               -0.000000                      0.0         -0.016211  \n",
      "6               -0.000000                      0.0    -147568.371469  \n",
      "10              -0.000000                      0.0         -0.000182  \n",
      "12              -0.051618                      0.0          0.002578  \n",
      "14              -0.000000                      0.0          0.001728  \n",
      "16              -0.043533                      0.0          0.001862  \n",
      "18              -0.000000                      0.0          0.001595  \n",
      "20              -0.000000                      0.0         -0.113006  \n",
      "22              -0.036966                      0.0          0.001353  \n",
      "24              -0.000000                      0.0          0.001232  \n",
      "26              -0.034526                      0.0          0.001185  \n",
      "28              -0.000000                      0.0          0.032982  \n",
      "31              -0.000000                      0.0         -0.001961  \n",
      "38              -0.047865                      0.0          0.002251  \n",
      "40              -0.000000                      0.0          0.006713  \n",
      "42              -0.000000                      0.0          0.490777  \n",
      "46              -0.000000                      0.0         -1.161417  \n",
      "48              -0.000000                      0.0         -0.725044  \n",
      "50              -0.000000                      0.0          0.031647  \n",
      "52              -0.000000                      0.0          0.000529  \n",
      "\n",
      "Residual statistics:\n",
      "count    2.228390e+05\n",
      "mean    -6.622471e-01\n",
      "std      3.126063e+02\n",
      "min     -1.475684e+05\n",
      "25%     -4.546266e-14\n",
      "50%     -2.894172e-16\n",
      "75%      2.883373e-08\n",
      "max      5.332723e-01\n",
      "Name: delta_u_residual, dtype: float64\n",
      "\n",
      "Rows with |residual| > 0.01: 1543 (0.69%)\n"
     ]
    }
   ],
   "source": [
    "# Show contribution terms\n",
    "contrib_cols = [\n",
    "    'event_type', 'delta_u_real', 'delta_u_pred',\n",
    "    'contrib_u_from_borrow', 'contrib_u_from_repay', 'contrib_u_from_liquidate',\n",
    "    'contrib_u_from_withdraw', 'contrib_u_from_supply', 'contrib_u_from_interest',\n",
    "    'delta_u_residual'\n",
    "]\n",
    "\n",
    "print(\"\\nUtilization Contribution Terms (rows with significant change):\")\n",
    "contrib_df = df[contrib_cols]\n",
    "significant = contrib_df[contrib_df['delta_u_real'].abs() > 0.001]\n",
    "print(significant.head(20))\n",
    "\n",
    "print(\"\\nResidual statistics:\")\n",
    "print(df['delta_u_residual'].describe())\n",
    "print(f\"\\nRows with |residual| > 0.01: {(df['delta_u_residual'].abs() > 0.01).sum()} ({100*(df['delta_u_residual'].abs() > 0.01).sum()/len(df):.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a785b8c4",
   "metadata": {},
   "source": [
    "## IRM Slope Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3452eacc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IRM Slope Analysis:\n",
      "\n",
      "Valid slope measurements: 203292 rows\n",
      "\n",
      "IRM slope statistics:\n",
      "count    2.032920e+05\n",
      "mean     1.122489e+03\n",
      "std      2.444427e+05\n",
      "min     -1.131314e+07\n",
      "25%     -2.401353e+00\n",
      "50%      1.314870e-01\n",
      "75%      2.167668e+00\n",
      "max      6.900629e+07\n",
      "Name: irm_slope, dtype: float64\n",
      "\n",
      "Sample rows with significant IRM slope:\n",
      "        event_type  delta_u_real  delta_borrow_apr     irm_slope  \\\n",
      "6           Borrow -3.043641e-03          0.039219 -1.288568e+01   \n",
      "8           Borrow -1.792826e-04         -0.000377  2.105185e+00   \n",
      "11  AccrueInterest  3.164238e-07          0.001011  3.196422e+03   \n",
      "13  AccrueInterest  1.213619e-08         -0.018030 -1.485617e+06   \n",
      "15  AccrueInterest  5.010309e-08          0.015434  3.080497e+05   \n",
      "17  AccrueInterest  1.244524e-08         -0.015627 -1.255660e+06   \n",
      "20          Borrow  1.029665e-03          0.014841  1.441321e+01   \n",
      "21  AccrueInterest  5.737837e-08          0.000464  8.087400e+03   \n",
      "23  AccrueInterest  1.262074e-08         -0.013399 -1.061698e+06   \n",
      "25  AccrueInterest  5.353901e-08          0.013096  2.446091e+05   \n",
      "\n",
      "    irm_response_to_withdraw  irm_response_to_repay  irm_response_to_liquidate  \n",
      "6                        NaN                    NaN                        NaN  \n",
      "8                        NaN                    NaN                        NaN  \n",
      "11                       NaN                    NaN                        NaN  \n",
      "13                       NaN                    NaN                        NaN  \n",
      "15                       NaN                    NaN                        NaN  \n",
      "17                       NaN                    NaN                        NaN  \n",
      "20                       NaN                    NaN                        NaN  \n",
      "21                       NaN                    NaN                        NaN  \n",
      "23                       NaN                    NaN                        NaN  \n",
      "25                       NaN                    NaN                        NaN  \n"
     ]
    }
   ],
   "source": [
    "# Analyze IRM slope\n",
    "irm_cols = [\n",
    "    'event_type', 'delta_u_real', 'delta_borrow_apr', 'irm_slope',\n",
    "    'irm_response_to_withdraw', 'irm_response_to_repay', 'irm_response_to_liquidate'\n",
    "]\n",
    "\n",
    "print(\"IRM Slope Analysis:\")\n",
    "irm_df = df[irm_cols]\n",
    "valid_slope = irm_df[irm_df['irm_slope'].notna()]\n",
    "\n",
    "if len(valid_slope) > 0:\n",
    "    print(f\"\\nValid slope measurements: {len(valid_slope)} rows\")\n",
    "    print(\"\\nIRM slope statistics:\")\n",
    "    print(valid_slope['irm_slope'].describe())\n",
    "    \n",
    "    print(\"\\nSample rows with significant IRM slope:\")\n",
    "    print(valid_slope[valid_slope['irm_slope'].abs() > 1.0].head(10))\n",
    "else:\n",
    "    print(\"No valid IRM slope measurements (no significant utilization changes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5cb98b",
   "metadata": {},
   "source": [
    "## Rolling Window Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a9017727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6-Hour Rolling Window Statistics:\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['util_max_6h', 'borrow_apr_max_6h'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      2\u001b[39m rolling_cols_6h = [\n\u001b[32m      3\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mblock_number\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mutilization_rate\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mutil_mean_6h\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mutil_max_6h\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mutil_std_6h\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      4\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mborrow_apr\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mborrow_apr_mean_6h\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mborrow_apr_max_6h\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      5\u001b[39m ]\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m6-Hour Rolling Window Statistics:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrolling_cols_6h\u001b[49m\u001b[43m]\u001b[49m.tail(\u001b[32m20\u001b[39m))\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Check if 24h columns exist\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mutil_mean_24h\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m df.columns:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/morpho_blue/.venv/lib/python3.12/site-packages/pandas/core/frame.py:4119\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4117\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[32m   4118\u001b[39m         key = \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m4119\u001b[39m     indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcolumns\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m]\n\u001b[32m   4121\u001b[39m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[32m   4122\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[38;5;28mbool\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/morpho_blue/.venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:6212\u001b[39m, in \u001b[36mIndex._get_indexer_strict\u001b[39m\u001b[34m(self, key, axis_name)\u001b[39m\n\u001b[32m   6209\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6210\u001b[39m     keyarr, indexer, new_indexer = \u001b[38;5;28mself\u001b[39m._reindex_non_unique(keyarr)\n\u001b[32m-> \u001b[39m\u001b[32m6212\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6214\u001b[39m keyarr = \u001b[38;5;28mself\u001b[39m.take(indexer)\n\u001b[32m   6215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[32m   6216\u001b[39m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/morpho_blue/.venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:6264\u001b[39m, in \u001b[36mIndex._raise_if_missing\u001b[39m\u001b[34m(self, key, indexer, axis_name)\u001b[39m\n\u001b[32m   6261\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   6263\u001b[39m not_found = \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask.nonzero()[\u001b[32m0\u001b[39m]].unique())\n\u001b[32m-> \u001b[39m\u001b[32m6264\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in index\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: \"['util_max_6h', 'borrow_apr_max_6h'] not in index\""
     ]
    }
   ],
   "source": [
    "# Show rolling window statistics for 6h and 24h\n",
    "rolling_cols_6h = [\n",
    "    'block_number', 'utilization_rate', 'util_mean_6h', 'util_max_6h', 'util_std_6h',\n",
    "    'borrow_apr', 'borrow_apr_mean_6h', 'borrow_apr_max_6h'\n",
    "]\n",
    "\n",
    "print(\"\\n6-Hour Rolling Window Statistics:\")\n",
    "print(df[rolling_cols_6h].tail(20))\n",
    "\n",
    "# Check if 24h columns exist\n",
    "if 'util_mean_24h' in df.columns:\n",
    "    rolling_cols_24h = [\n",
    "        'block_number', 'utilization_rate', 'util_mean_24h', 'util_max_24h',\n",
    "        'borrow_apr', 'borrow_apr_mean_24h', 'borrow_apr_max_24h'\n",
    "    ]\n",
    "    print(\"\\n24-Hour Rolling Window Statistics:\")\n",
    "    print(df[rolling_cols_24h].tail(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b1cc192e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6-Hour Flow Intensities (assets per second):\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['liquidate_intensity_6h'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      2\u001b[39m intensity_cols = [\n\u001b[32m      3\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mborrow_intensity_6h\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mrepay_intensity_6h\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mliquidate_intensity_6h\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      4\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mwithdraw_intensity_6h\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33msupply_intensity_6h\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33minterest_intensity_6h\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      5\u001b[39m ]\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m6-Hour Flow Intensities (assets per second):\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mintensity_cols\u001b[49m\u001b[43m]\u001b[49m.describe())\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Normalized intensities\u001b[39;00m\n\u001b[32m     11\u001b[39m norm_cols = [\n\u001b[32m     12\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mborrow_intensity_norm_supply_6h\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     13\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mwithdraw_intensity_norm_supply_6h\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     14\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mrepay_intensity_norm_supply_6h\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     15\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/morpho_blue/.venv/lib/python3.12/site-packages/pandas/core/frame.py:4119\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4117\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[32m   4118\u001b[39m         key = \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m4119\u001b[39m     indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcolumns\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m]\n\u001b[32m   4121\u001b[39m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[32m   4122\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[38;5;28mbool\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/morpho_blue/.venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:6212\u001b[39m, in \u001b[36mIndex._get_indexer_strict\u001b[39m\u001b[34m(self, key, axis_name)\u001b[39m\n\u001b[32m   6209\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6210\u001b[39m     keyarr, indexer, new_indexer = \u001b[38;5;28mself\u001b[39m._reindex_non_unique(keyarr)\n\u001b[32m-> \u001b[39m\u001b[32m6212\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6214\u001b[39m keyarr = \u001b[38;5;28mself\u001b[39m.take(indexer)\n\u001b[32m   6215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[32m   6216\u001b[39m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/morpho_blue/.venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:6264\u001b[39m, in \u001b[36mIndex._raise_if_missing\u001b[39m\u001b[34m(self, key, indexer, axis_name)\u001b[39m\n\u001b[32m   6261\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   6263\u001b[39m not_found = \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask.nonzero()[\u001b[32m0\u001b[39m]].unique())\n\u001b[32m-> \u001b[39m\u001b[32m6264\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in index\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: \"['liquidate_intensity_6h'] not in index\""
     ]
    }
   ],
   "source": [
    "# Flow intensities\n",
    "intensity_cols = [\n",
    "    'borrow_intensity_6h', 'repay_intensity_6h', 'liquidate_intensity_6h',\n",
    "    'withdraw_intensity_6h', 'supply_intensity_6h', 'interest_intensity_6h'\n",
    "]\n",
    "\n",
    "print(\"\\n6-Hour Flow Intensities (assets per second):\")\n",
    "print(df[intensity_cols].describe())\n",
    "\n",
    "# Normalized intensities\n",
    "norm_cols = [\n",
    "    'borrow_intensity_norm_supply_6h',\n",
    "    'withdraw_intensity_norm_supply_6h',\n",
    "    'repay_intensity_norm_supply_6h'\n",
    "]\n",
    "\n",
    "print(\"\\n6-Hour Normalized Intensities (fraction of supply per second):\")\n",
    "print(df[norm_cols].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24029488",
   "metadata": {},
   "source": [
    "## Attribution Aggregates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416cbd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attribution contribution sums\n",
    "attribution_sum_cols = [\n",
    "    'contrib_withdraw_sum_6h', 'contrib_repay_sum_6h', 'contrib_liquidate_sum_6h',\n",
    "    'contrib_borrow_sum_6h', 'contrib_supply_sum_6h'\n",
    "]\n",
    "\n",
    "print(\"\\n6-Hour Attribution Contribution Sums:\")\n",
    "print(df[attribution_sum_cols].describe())\n",
    "\n",
    "# Check latest values\n",
    "print(\"\\nLatest attribution sums (6h window):\")\n",
    "print(df[attribution_sum_cols].iloc[-1])\n",
    "\n",
    "# Absolute contribution sums (activity magnitude)\n",
    "abs_sum_cols = [\n",
    "    'contrib_withdraw_abs_sum_6h',\n",
    "    'contrib_repay_abs_sum_6h',\n",
    "    'contrib_liquidate_abs_sum_6h'\n",
    "]\n",
    "\n",
    "print(\"\\n6-Hour Absolute Contribution Sums (activity magnitude):\")\n",
    "print(df[abs_sum_cols].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e276f590",
   "metadata": {},
   "source": [
    "## Volatility Attribution Shares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8e7982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Volatility attribution shares (6h window)\n",
    "vol_share_cols = [\n",
    "    'vol_share_withdraw_6h', 'vol_share_repay_6h', 'vol_share_liquidate_6h',\n",
    "    'vol_share_borrow_6h', 'vol_share_supply_6h'\n",
    "]\n",
    "\n",
    "if all(col in df.columns for col in vol_share_cols):\n",
    "    print(\"\\nVolatility Attribution Shares (6h window):\")\n",
    "    print(\"These show what fraction of utilization variance is explained by each flow type.\")\n",
    "    print(df[vol_share_cols].describe())\n",
    "    \n",
    "    # Show rows with defined shares\n",
    "    valid_shares = df[vol_share_cols].dropna(how='all')\n",
    "    if len(valid_shares) > 0:\n",
    "        print(\"\\nLatest volatility shares:\")\n",
    "        print(valid_shares.tail(10))\n",
    "    else:\n",
    "        print(\"\\nNo valid volatility shares (insufficient variance in window)\")\n",
    "else:\n",
    "    print(\"\\nVolatility share columns not found in dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb43f7d0",
   "metadata": {},
   "source": [
    "## Expanding Statistics (All-Time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e08261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expanding window statistics\n",
    "expanding_cols = [\n",
    "    'util_mean_expanding', 'util_std_expanding', 'util_max_expanding',\n",
    "    'borrow_apr_mean_expanding', 'borrow_apr_std_expanding', 'borrow_apr_max_expanding',\n",
    "    'delta_u_std_expanding'\n",
    "]\n",
    "\n",
    "print(\"\\nExpanding (All-Time) Statistics:\")\n",
    "print(df[expanding_cols].tail(20))\n",
    "\n",
    "print(\"\\nFinal expanding statistics:\")\n",
    "print(df[expanding_cols].iloc[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487910b3",
   "metadata": {},
   "source": [
    "## Data Quality Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7d5e7629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA QUALITY SUMMARY\n",
      "\n",
      "1. Integer Column Integrity:\n",
      "   delta_supply_assets: 0 nulls\n",
      "   delta_borrow_assets: 0 nulls\n",
      "   total_supplied_assets: 0 nulls\n",
      "   outstanding_borrow_assets: 0 nulls\n",
      "\n",
      "2. Utilization Bounds [0, 1]:\n",
      "   Min: 0.000000\n",
      "   Max: 1.000000\n",
      "   Out of bounds: 0 rows\n",
      "\n",
      "3. Attribution Residuals:\n",
      "   Mean: 0.662608\n",
      "   Median: 0.000000\n",
      "   95th percentile: 0.000906\n",
      "   Max: 147568.371469\n",
      "   Rows with |residual| > 0.01: 1543 (0.69%)\n",
      "\n",
      "4. Flow Decomposition Identity Check:\n",
      "   Max difference: 0.00\n",
      "   Mean difference: 0.00e+00\n",
      "   Rows with diff > 1.0: 0\n",
      "\n",
      "============================================================\n",
      "✓ Attribution feature dataset ready for analysis\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Summary statistics for validation\n",
    "print(\"DATA QUALITY SUMMARY\")\n",
    "\n",
    "\n",
    "# 1. Check integer columns\n",
    "int_cols = [\n",
    "    'delta_supply_assets', 'delta_borrow_assets',\n",
    "    'total_supplied_assets', 'outstanding_borrow_assets'\n",
    "]\n",
    "print(\"\\n1. Integer Column Integrity:\")\n",
    "for col in int_cols:\n",
    "    nulls = df[col].isna().sum()\n",
    "    print(f\"   {col}: {nulls} nulls\")\n",
    "\n",
    "# 2. Utilization bounds\n",
    "print(\"\\n2. Utilization Bounds [0, 1]:\")\n",
    "util = df['utilization_rate']\n",
    "print(f\"   Min: {util.min():.6f}\")\n",
    "print(f\"   Max: {util.max():.6f}\")\n",
    "print(f\"   Out of bounds: {((util < 0) | (util > 1)).sum()} rows\")\n",
    "\n",
    "# 3. Residual distribution\n",
    "print(\"\\n3. Attribution Residuals:\")\n",
    "residuals = df['delta_u_residual'].abs()\n",
    "print(f\"   Mean: {residuals.mean():.6f}\")\n",
    "print(f\"   Median: {residuals.median():.6f}\")\n",
    "print(f\"   95th percentile: {residuals.quantile(0.95):.6f}\")\n",
    "print(f\"   Max: {residuals.max():.6f}\")\n",
    "print(f\"   Rows with |residual| > 0.01: {(residuals > 0.01).sum()} ({100*(residuals > 0.01).sum()/len(df):.2f}%)\")\n",
    "\n",
    "# 4. Flow decomposition check\n",
    "print(\"\\n4. Flow Decomposition Identity Check:\")\n",
    "reconstructed = (\n",
    "    df['borrow_in_assets'] - df['repay_out_assets'] -\n",
    "    df['liquidate_repay_assets'] + df['interest_assets']\n",
    ")\n",
    "delta_actual = df['delta_borrow_assets'].astype('float64')\n",
    "flow_diff = (reconstructed - delta_actual).abs()\n",
    "print(f\"   Max difference: {flow_diff.max():.2f}\")\n",
    "print(f\"   Mean difference: {flow_diff.mean():.2e}\")\n",
    "print(f\"   Rows with diff > 1.0: {(flow_diff > 1.0).sum()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"✓ Attribution feature dataset ready for analysis\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673f6f6a",
   "metadata": {},
   "source": [
    "## Export Dataset\n",
    "\n",
    "Save the attribution feature table for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32470ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to parquet\n",
    "output_path = Path.cwd().parent / \"data_examples\" / \"attribution_features.parquet\"\n",
    "df.to_parquet(output_path, index=False, engine='pyarrow', compression='snappy')\n",
    "\n",
    "print(f\"✓ Saved attribution features to: {output_path}\")\n",
    "print(f\"  Size: {output_path.stat().st_size / 1024 / 1024:.2f} MB\")\n",
    "print(f\"  Rows: {len(df):,}\")\n",
    "print(f\"  Columns: {len(df.columns)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "morpho-blue",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
